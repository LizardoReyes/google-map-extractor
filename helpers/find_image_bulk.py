from pathlib import Path
import requests
import hashlib
import openai
import os
import replicate
from PIL import Image
from io import BytesIO
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Claves API
VERSION = 1
PEXELS_API_KEY = os.getenv("PEXELS_API_KEY_" + str(VERSION))
UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY_" + str(VERSION))
PIXABAY_API_KEY = os.getenv("PIXABAY_API_KEY_" + str(VERSION))
REPLICATE_API_TOKEN = os.getenv("REPLICATE_API_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Autenticación
replicate.Client(api_token=REPLICATE_API_TOKEN)

BASE_DIR = Path(__file__).resolve().parent.parent
PATH_IMAGES = BASE_DIR / "images"

if not all([PEXELS_API_KEY, UNSPLASH_ACCESS_KEY, PIXABAY_API_KEY]):
    raise EnvironmentError("❌ Alguna clave API no está definida en el archivo .env")
print("✅ Todas las claves API están definidas correctamente.")


# Cost black-forest-labs/flux-schnell : $0.003000 / image
# Cost stability-ai/sdxl : $0.000975 / second (1 image ~ 3 seconds)
#def generate_image_replicate(prompt: str, image_name: str, model: str = "black-forest-labs/flux-schnell", version: str = "") -> str:
def generate_image_replicate(prompt: str, image_name: str, model: str = "stability-ai/sdxl", version: str = "7762fd07cf82c948538e41f63f77d685e02b063e37e496e96eefd46c929f9bdc") -> str:
    """
    Generate an image using Replicate and save it locally.

    Args:
        prompt (str): Text prompt to generate the image.
        image_name (str): Output image file name.
        model (str): Replicate model to use (default: Stable Diffusion XL).
        version (str): Specific version ID of the model.

    Returns:
        str: Path to saved image.
    """
    output = replicate.run(
        f"{model}:{version}" if version else model,
        input={"prompt": prompt, "width": 768, "height": 512}
    )

    if isinstance(output, list) and output:
        image_url = output[0]

        Path("images").mkdir(parents=True, exist_ok=True)
        image_path = f"images/{image_name}"

        response = requests.get(image_url)
        if response.status_code == 200:
            with open(image_path, "wb") as f:
                f.write(response.content)
            return image_path
        else:
            print(f"Image download failed: {response.status_code}")
    else:
        print("No image generated by Replicate.")
    return ""


def download_image_google_scraping(query: str, image_name: str, min_width=162, min_height=121) -> str:
    image_path = PATH_IMAGES / image_name
    if image_path.exists():
        return str(image_path)

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        search_url = f"https://www.google.com/search?tbm=isch&q={requests.utils.quote(query)}"
        response = requests.get(search_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        images = soup.select("img")

        if len(images) < 2:
            print("[GoogleScraping] No se encontraron imágenes.")
            return ""

        # Probar imágenes hasta encontrar una con tamaño aceptable
        for img in images[1:]:
            img_url = img.get("src")
            if not img_url or not img_url.startswith("http"):
                continue

            try:
                img_response = requests.get(img_url, headers=headers, timeout=10)
                img_response.raise_for_status()
                img_data = BytesIO(img_response.content)
                with Image.open(img_data) as im:
                    width, height = im.size
                    #print(f"[GoogleScraping] Probando imagen: {img_url} - Tamaño: {width}x{height}")
                    if width >= min_width and height >= min_height:
                        PATH_IMAGES.mkdir(parents=True, exist_ok=True)
                        with open(image_path, "wb") as f:
                            f.write(img_response.content)
                        return str(image_path)
            except Exception as e:
                print(f"[GoogleScraping] Error probando imagen: {e}")
                continue

        print("[GoogleScraping] No se encontraron imágenes que cumplan con el tamaño mínimo.")
    except Exception as e:
        print(f"[GoogleScraping] Error general: {e}")
    return ""


def download_image_lorem_picsum(query: str, image_name: str, width=1280, height=720) -> str:
    image_path = PATH_IMAGES / image_name
    if image_path.exists():
        return str(image_path)

    seed = hashlib.md5(query.encode("utf-8")).hexdigest()
    url = f"https://picsum.photos/seed/{seed}/{width}/{height}"

    try:
        PATH_IMAGES.mkdir(parents=True, exist_ok=True)
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        with open(image_path, "wb") as file:
            file.write(response.content)
        return str(image_path)
    except Exception as e:
        print(f"[LoremPicsum] Error: {e}")
    return ""


def download_image_pixabay(query: str, image_name: str) -> str:
    image_path = PATH_IMAGES / image_name
    if image_path.exists():
        return str(image_path)

    url = "https://pixabay.com/api/"
    params = {
        "key": PIXABAY_API_KEY,
        "q": query,
        "image_type": "photo",
        "orientation": "horizontal",
        "per_page": 3,
    }

    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        if not data.get("hits"):
            print(f"[Pixabay] No results for: {query}")
            return ""
        image_url = data["hits"][0]["largeImageURL"]
        PATH_IMAGES.mkdir(parents=True, exist_ok=True)
        img_response = requests.get(image_url, timeout=10)
        img_response.raise_for_status()
        with open(image_path, "wb") as file:
            file.write(img_response.content)
        return str(image_path)
    except Exception as e:
        print(f"[Pixabay] Error: {e}")
    return ""


def download_image_unsplash(query: str, image_name: str) -> str:
    image_path = PATH_IMAGES / image_name
    if image_path.exists():
        return str(image_path)

    url = "https://api.unsplash.com/search/photos"
    headers = {"Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"}
    params = {"query": query, "orientation": "landscape", "per_page": 1, "page": 1}

    try:
        resp = requests.get(url, headers=headers, params=params, timeout=10)
        resp.raise_for_status()
        results = resp.json().get("results", [])
        if not results:
            print(f"[Unsplash] No results for: {query}")
            return ""
        image_url = results[0]["urls"]["regular"]
        PATH_IMAGES.mkdir(parents=True, exist_ok=True)
        img_resp = requests.get(image_url, timeout=10)
        img_resp.raise_for_status()
        with open(image_path, "wb") as f:
            f.write(img_resp.content)
        return str(image_path)
    except Exception as e:
        print(f"[Unsplash] Error: {e}")
    return ""


def download_image_pexels(query: str, image_name: str) -> str:
    image_path = PATH_IMAGES / image_name
    if image_path.exists():
        return str(image_path)

    url = "https://api.pexels.com/v1/search"
    headers = {"Authorization": PEXELS_API_KEY}
    params = {"query": query, "orientation": "landscape", "per_page": 1, "page": 1}

    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        if not data.get("photos"):
            print(f"[Pexels] No results for: {query}")
            return ""
        image_url = data["photos"][0]["src"]["large"]
        PATH_IMAGES.mkdir(parents=True, exist_ok=True)
        img_response = requests.get(image_url, timeout=10)
        img_response.raise_for_status()
        with open(image_path, "wb") as file:
            file.write(img_response.content)
        return str(image_path)
    except Exception as e:
        print(f"[Pexels] Error: {e}")
    return ""


def download_image_api(query: str, image_name: str, index: int = None, total: int = None) -> str:
    image_path = PATH_IMAGES / image_name
    if image_path.exists():
        print(f"📁 ({index}/{total}) Imagen ya existe: {image_name}" if index else f"📁 Imagen ya existe: {image_name}")
        return str(image_path)

    for func in [
        download_image_pexels,
        download_image_unsplash,
        download_image_pixabay,
        download_image_google_scraping,
        #generate_image_replicate,
        #download_image_lorem_picsum,
    ]:
        path = func(query, image_name)
        if path:
            print(
                f"✅ ({index}/{total}) Imagen encontrada para '{query}' usando {func.__name__}" if index else f"✅ Imagen encontrada para '{query}' usando {func.__name__}")
            return path

    print(
        f"❌ ({index}/{total}) No se encontró imagen para: {query}" if index else f"❌ No se encontró imagen para: {query}")
    return ""


def find_image_bulk(queries: list, image_names: list) -> list:
    if len(queries) != len(image_names):
        raise ValueError("The length of queries and image_names must be the same.")

    total = len(queries)
    results = []
    for idx, (q, img) in enumerate(zip(queries, image_names), start=1):
        results.append(download_image_api(q, img, idx, total))
    return results


def main2():
    base_query = "Create an image for the business: ${name} in the city: ${city} with the categories: ${categories}"
    query = base_query.format(name="Sagres Campo Pequeno", city="Lisboa",
                               categories="sala de concertos, local para eventos")
    image_name = "rsagres-campo-pequeno.jpg"
    image_path = generate_image_replicate(query, image_name)

def main():
    query = ["Wok To Walk - Pizza - Kebab", "NuPoo Pizza&Burger Vršovice", "Shell"]
    image_name = ["restaurant_food.jpg", "landscape.jpg", "city_skyline.jpg"]
    image_path = find_image_bulk(query, image_name)

if __name__ == "__main__":
    #main()
    main2()
